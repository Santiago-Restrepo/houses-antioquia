{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "from concurrent.futures import ThreadPoolExecutor, as_completed\n",
    "import time\n",
    "from typing import List, Dict\n",
    "from urllib.parse import parse_qs, urlparse\n",
    "import json\n",
    "import re"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Definitions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "class WebScraper:\n",
    "    def __init__(self, base_url: str, max_workers: int = 5, retries: int = 3, timeout: int = 10):\n",
    "        \"\"\"\n",
    "        Initialize the web scraper.\n",
    "\n",
    "        :param base_url: The base URL to scrape.\n",
    "        :param max_workers: The number of threads for concurrent scraping.\n",
    "        :param retries: Number of retries for failed requests.\n",
    "        :param timeout: Timeout for each request in seconds.\n",
    "        \"\"\"\n",
    "        self.base_url = base_url\n",
    "        self.max_workers = max_workers\n",
    "        self.retries = retries\n",
    "        self.timeout = timeout\n",
    "        self.headers = {\n",
    "            'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36'\n",
    "        }\n",
    "\n",
    "    def fetch_page(self, url: str) -> str:\n",
    "        \"\"\"\n",
    "        Fetch a page and return its HTML content.\n",
    "\n",
    "        :param url: The URL of the page.\n",
    "        :return: The HTML content of the page.\n",
    "        \"\"\"\n",
    "        for attempt in range(self.retries):\n",
    "            try:\n",
    "                response = requests.get(url, headers=self.headers, timeout=self.timeout)\n",
    "                response.raise_for_status()\n",
    "                return response.text\n",
    "            except requests.exceptions.RequestException as e:\n",
    "                print(f\"Attempt {attempt + 1} failed for URL: {url}, Error: {e}\")\n",
    "                time.sleep(1)  # Backoff before retry\n",
    "        return \"\"\n",
    "\n",
    "    def parse_page(self, html_content: str) -> Dict:\n",
    "        \"\"\"\n",
    "        Parse the HTML content and extract the desired data.\n",
    "\n",
    "        :param html_content: The HTML content of the page.\n",
    "        :return: A dictionary with the extracted data.\n",
    "        \"\"\"\n",
    "        soup = BeautifulSoup(html_content, 'lxml')\n",
    "        # Example: Extract all links and their text\n",
    "        data = {\n",
    "            'title': soup.title.string if soup.title else 'No title',\n",
    "            'links': [(a.get('href'), a.text) for a in soup.find_all('a', href=True)]\n",
    "        }\n",
    "        return data\n",
    "    \n",
    "\n",
    "    def scrape_pages(self, urls: List[str]) -> List[Dict]:\n",
    "        \"\"\"\n",
    "        Scrape multiple pages concurrently.\n",
    "\n",
    "        :param urls: A list of URLs to scrape.\n",
    "        :return: A list of dictionaries with the extracted data.\n",
    "        \"\"\"\n",
    "        results = []\n",
    "        with ThreadPoolExecutor(max_workers=self.max_workers) as executor:\n",
    "            future_to_url = {executor.submit(self.fetch_page, url): url for url in urls}\n",
    "            for future in as_completed(future_to_url):\n",
    "                url = future_to_url[future]\n",
    "                try:\n",
    "                    html_content = future.result()\n",
    "                    if html_content:\n",
    "                        data = self.parse_page(html_content)\n",
    "                        results.append({url: data})\n",
    "                except Exception as e:\n",
    "                    print(f\"Error fetching {url}: {e}\")\n",
    "        return results\n",
    "\n",
    "    def scrape_single_page(self, url: str) -> Dict:\n",
    "        \"\"\"\n",
    "        Scrape a single page.\n",
    "\n",
    "        :param url: The URL of the page.\n",
    "        :return: A dictionary with the extracted data.\n",
    "        \"\"\"\n",
    "        html_content = self.fetch_page(url)\n",
    "        if html_content:\n",
    "            return self.parse_page(html_content)\n",
    "        return {}\n",
    "    \n",
    "    def parse_city_page(self, html_content: str) -> list:\n",
    "        \"\"\"\n",
    "        Parses the page to extract all property listings.\n",
    "        :param html_content: The HTML content of the page.\n",
    "        :return: A list of dictionaries containing property details.\n",
    "        \"\"\"\n",
    "        soup = BeautifulSoup(html_content, 'lxml')\n",
    "        listings = []\n",
    "\n",
    "        # Find all divs with class \"row\" which represent listings\n",
    "        listing_divs = soup.find_all('div', class_='row')\n",
    "        for listing in listing_divs:\n",
    "            parsed_listing = self.parse_listing(listing)\n",
    "            if parsed_listing:\n",
    "                listings.append(parsed_listing)\n",
    "\n",
    "        return listings\n",
    "    \n",
    "    def parse_listing(self, listing) -> dict:\n",
    "        \"\"\"\n",
    "        Parses a single property listing and extracts details.\n",
    "        :param listing: A BeautifulSoup element containing the listing HTML.\n",
    "        :return: A dictionary with the extracted information.\n",
    "        \"\"\"\n",
    "        try:\n",
    "            # Extract the listing ID\n",
    "            id_match = re.search(r'Ficha\\.asp\\?xId=(\\d+)', str(listing))\n",
    "            listing_id = id_match.group(1) if id_match else 'N/A'\n",
    "\n",
    "            # Extract image URL\n",
    "            img_tag = listing.find('img')\n",
    "            img_url = img_tag['src'] if img_tag else 'N/A'\n",
    "            \n",
    "            # Extract neighborhood from subtitle\n",
    "            subtitle_tag = listing.find('p')\n",
    "            subtitle = subtitle_tag.text.strip() if subtitle_tag else 'N/A'\n",
    "            neighborhood_match = re.search(r'Anuncio \\d+ - (.+)', subtitle)\n",
    "            neighborhood = neighborhood_match.group(1) if neighborhood_match else 'N/A'\n",
    "\n",
    "            # Extract price and convert to numeric\n",
    "            price_tag = listing.find('h3', string=re.compile(r'\\$'))\n",
    "            price_text = price_tag.text.strip() if price_tag else 'N/A'\n",
    "            price_numeric = re.sub(r'[^\\d]', '', price_text)  # Remove non-numeric characters\n",
    "\n",
    "            # Extract rooms, bathrooms, parking, and area from the second <h3> tag\n",
    "            h3_tags = listing.find_all('h3')\n",
    "            if len(h3_tags) > 1:\n",
    "                second_h3 = h3_tags[1]\n",
    "                spans = second_h3.find_all('span')\n",
    "            else:\n",
    "                spans = []\n",
    "\n",
    "            rooms = spans[0].text.strip() if len(spans) > 0 else 'N/A'\n",
    "            bathrooms = spans[1].text.strip() if len(spans) > 1 else 'N/A'\n",
    "            parkings = spans[2].text.strip() if len(spans) > 2 else 'N/A'\n",
    "            area = re.sub(r'[^\\d]', '', spans[3].text.strip().split('M2')[0]) if len(spans) > 3 else 'N/A'  # Remove non-numeric characters for area\n",
    "\n",
    "            # Extract the largest <p> tag for the description\n",
    "            p_tags = listing.find_all('p')\n",
    "            description = max((p.text.strip() for p in p_tags if p.text.strip()), key=len, default='N/A')\n",
    "\n",
    "            return {\n",
    "                'id': listing_id,\n",
    "                'url': f\"{self.base_url}/Ficha.asp?xId={listing_id}\",\n",
    "                'neighborhood': neighborhood,\n",
    "                'img_url': img_url,\n",
    "                'price': int(price_numeric) if price_numeric else 'N/A',\n",
    "                'rooms': int(rooms) if rooms.isdigit() else 'N/A',\n",
    "                'bathrooms': int(bathrooms) if bathrooms.isdigit() else 'N/A',\n",
    "                'parkings': int(parkings) if parkings.isdigit() else 'N/A',\n",
    "                'area': int(area) if area.isdigit() else 'N/A',\n",
    "                'description': description\n",
    "            }\n",
    "        except Exception as e:\n",
    "            print(f\"Error parsing listing: {e}\")\n",
    "            return {}\n",
    "    \n",
    "    def scrape_city(self, pCiudad: int, start_offset: int = 0) -> list:\n",
    "        \"\"\"\n",
    "        Iterates through all pages for a given city and scrapes all listings.\n",
    "        Stops when less than 51 listings (divs with class \"row\") are found.\n",
    "        \n",
    "        :param pCiudad: The city code to scrape.\n",
    "        :param start_offset: The starting offset for pagination.\n",
    "        :return: A list of all scraped listings.\n",
    "        \"\"\"\n",
    "        all_listings = []\n",
    "        offset = start_offset\n",
    "        max_listings_per_page = 50\n",
    "\n",
    "        while True:\n",
    "            # Construct the URL with the current offset\n",
    "            rental_base_url = 'Resumen_Ciudad_arriendos.asp'\n",
    "            url = f\"{self.base_url}/{rental_base_url}?pCiudad={pCiudad}&pTipoInmueble=&offset={offset}\"\n",
    "            print(f\"Scraping: {url}\")\n",
    "\n",
    "            # Fetch and parse the page\n",
    "            html_content = self.fetch_page(url)\n",
    "            listings = self.parse_city_page(html_content)\n",
    "            \n",
    "            # Add the listings to the total list\n",
    "            all_listings.extend(listings)\n",
    "            \n",
    "            # Stop if fewer than 51 listings found (end of pagination)\n",
    "            if len(listings) < max_listings_per_page:\n",
    "                break\n",
    "            \n",
    "            # Increase offset for the next page\n",
    "            offset += max_listings_per_page\n",
    "\n",
    "        return all_listings\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initialization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_url = 'https://www.espaciourbano.com'\n",
    "scraper = WebScraper(base_url=base_url, max_workers=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Initial extraction of cities\n",
    "\n",
    "By first we are going to extract initial page which contains all the cities we could extract information"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Single page scrape\n",
    "data_fetched = scraper.scrape_single_page('https://www.espaciourbano.com/listado_arriendos.asp')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "we define a method for extracting code and name of the cities available from the page"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_city_info_urllib(url: str) -> dict:\n",
    "    \"\"\"\n",
    "    Extract the city code (pCiudad) and city name (nCiudad) using urllib.\n",
    "    \n",
    "    :param url: The URL string containing the city information.\n",
    "    :return: A dictionary with the city code and name.\n",
    "    \"\"\"\n",
    "    # Parse query parameters\n",
    "    parsed_url = urlparse(url)\n",
    "    query_params = parse_qs(parsed_url.query)\n",
    "    \n",
    "    # Extract city code and name\n",
    "    city_info = {\n",
    "        'name': query_params.get('pCiudad', [''])[0],\n",
    "        'code': query_params.get('nCiudad', [''])[0]\n",
    "    }\n",
    "    \n",
    "    return city_info\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_to_json(data: dict, file_name: str) -> None:\n",
    "    \"\"\"\n",
    "    Save the extracted city info into a JSON file.\n",
    "    \n",
    "    :param data: The dictionary containing the city info.\n",
    "    :param file_name: The name of the JSON file to save the data.\n",
    "    \"\"\"\n",
    "    with open(file_name, 'w') as json_file:\n",
    "        json.dump(data, json_file, indent=4)\n",
    "    print(f\"Data saved to {file_name}\")\n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data saved to data/cities.json\n"
     ]
    }
   ],
   "source": [
    "cities_base_link = '/Resumen_Ciudad_arriendos'\n",
    "cities_links = [link for link in data_fetched['links'] if link[0].startswith(cities_base_link)]\n",
    "\n",
    "cities = [extract_city_info_urllib(link[0]) for link in cities_links]\n",
    "\n",
    "save_to_json(cities, 'data/cities.json')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Then we need to iterate trhough citites to extract announcements"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scraping: https://www.espaciourbano.com/Resumen_Ciudad_arriendos.asp?pCiudad=10000&pTipoInmueble=&offset=0\n",
      "Scraping: https://www.espaciourbano.com/Resumen_Ciudad_arriendos.asp?pCiudad=10000&pTipoInmueble=&offset=50\n",
      "Scraping: https://www.espaciourbano.com/Resumen_Ciudad_arriendos.asp?pCiudad=10000&pTipoInmueble=&offset=100\n",
      "Scraping: https://www.espaciourbano.com/Resumen_Ciudad_arriendos.asp?pCiudad=10000&pTipoInmueble=&offset=150\n",
      "Scraping: https://www.espaciourbano.com/Resumen_Ciudad_arriendos.asp?pCiudad=10000&pTipoInmueble=&offset=200\n",
      "Scraping: https://www.espaciourbano.com/Resumen_Ciudad_arriendos.asp?pCiudad=10000&pTipoInmueble=&offset=250\n",
      "Scraping: https://www.espaciourbano.com/Resumen_Ciudad_arriendos.asp?pCiudad=10000&pTipoInmueble=&offset=300\n",
      "Scraping: https://www.espaciourbano.com/Resumen_Ciudad_arriendos.asp?pCiudad=10000&pTipoInmueble=&offset=350\n",
      "Scraping: https://www.espaciourbano.com/Resumen_Ciudad_arriendos.asp?pCiudad=10000&pTipoInmueble=&offset=400\n",
      "Scraping: https://www.espaciourbano.com/Resumen_Ciudad_arriendos.asp?pCiudad=10000&pTipoInmueble=&offset=450\n",
      "Data saved to data/cities/10000.json\n"
     ]
    }
   ],
   "source": [
    "def extract_rentals_info():\n",
    "  for city in cities:\n",
    "    "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
